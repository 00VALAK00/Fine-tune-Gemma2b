{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-08-04T17:53:16.641007Z",
     "start_time": "2024-08-04T17:53:07.090298Z"
    }
   },
   "source": [
    "!pip3 install -q -U bitsandbytes\n",
    "!pip3 install -q -U peft\n",
    "!pip3 install -q -U trl\n",
    "!pip3 install -q -U accelerate\n",
    "!pip3 install -q -U datasets\n",
    "!pip3 install -q -U transformers"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T17:53:17.036190Z",
     "start_time": "2024-08-04T17:53:16.641964Z"
    }
   },
   "id": "1320e050daa92227",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1469eed8e72d445f90855a054e820aab"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import  AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "device=torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "model_id=\"google/gemma-2-2b\"\n",
    "bnb_config_4bit=BitsAndBytesConfig(load_in_4bit=True,\n",
    "                                   bnb_4bit_use_double_quant=True,\n",
    "                                   bnb_4bit_quant_type=\"nf4\",\n",
    "                                   bnb_4bit_compute_dtype=torch.float16)\n",
    "\n",
    "bnb_config_8bit=BitsAndBytesConfig(load_in_8bit=True)\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T08:42:26.476349Z",
     "start_time": "2024-08-05T08:42:26.471599Z"
    }
   },
   "id": "8b80450f3deccc73",
   "outputs": [],
   "execution_count": 14
  },
  {
   "cell_type": "code",
   "source": [
    "model=AutoModelForCausalLM.from_pretrained(model_id,\n",
    "                                           device_map=\"auto\",\n",
    "                                           quantization_config=bnb_config_4bit,\n",
    "                                           attn_implementation='eager'\n",
    "                                           )\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_id)\n",
    "tokenizer.pad_token=tokenizer.eos_token\n",
    "tokenizer.padding_side=\"right\"\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T08:42:51.700653Z",
     "start_time": "2024-08-05T08:42:27.368342Z"
    }
   },
   "id": "5c895464a93f0ac",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f478f7796849438e90ea8ff571111319"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "source": [
    "#prompt=\"what dinner should I have today?\"\n",
    "#input_ids=tokenizer(prompt,return_tensors=\"pt\").input_ids.to(device)\n",
    "#with torch.no_grad():\n",
    "    #output_ids=model.generate(input_ids,max_length=300,do_sample=True,top_k=2)\n",
    "    \n",
    "#output=tokenizer.decode(output_ids[0],skip_special_tokens=True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-04T17:53:57.921712Z",
     "start_time": "2024-08-04T17:53:57.919219Z"
    }
   },
   "id": "59d6b4738461a4a0",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T17:53:57.928137Z",
     "start_time": "2024-08-04T17:53:57.924242Z"
    }
   },
   "cell_type": "code",
   "source": "# number of params\n",
   "id": "8ec30a0335f42757",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:03.651535Z",
     "start_time": "2024-08-05T08:43:03.645282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Get the target modules for lora config\n",
    "import bitsandbytes\n",
    "\n",
    "def get_modules(model):\n",
    "    modules=set()\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module,bitsandbytes.nn.Linear4bit):\n",
    "            names=name.split(\".\")\n",
    "            modules.add(names[-1] if len(name)>1 else names[0])\n",
    "    return modules\n",
    "\n",
    "target_modules=get_modules(model)   \n"
   ],
   "id": "7392308e8cf94ce4",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:10.618488Z",
     "start_time": "2024-08-05T08:43:10.613980Z"
    }
   },
   "cell_type": "code",
   "source": "target_modules",
   "id": "8bed634638c58e2d",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'down_proj', 'gate_proj', 'k_proj', 'o_proj', 'q_proj', 'up_proj', 'v_proj'}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "source": [
    "# Lora adapters\n",
    "from peft import get_peft_model,LoraConfig,prepare_model_for_kbit_training,TaskType\n",
    "\n",
    "lora_config=LoraConfig(r=16,\n",
    "    target_modules=target_modules,\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:04.406877Z",
     "start_time": "2024-08-05T08:43:04.401094Z"
    }
   },
   "id": "842d6ab7665aa284",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:05.826465Z",
     "start_time": "2024-08-05T08:43:05.300420Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model=get_peft_model(model,lora_config)",
   "id": "cad0308fd844c8e",
   "outputs": [],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:05.909247Z",
     "start_time": "2024-08-05T08:43:05.898095Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model.print_trainable_parameters()",
   "id": "52fb11583f364d77",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,766,720 || all params: 2,635,108,608 || trainable%: 0.7881\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare dataset for Instruction Finetuning\n",
   "id": "8d577b3455dd6da1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:19.682829Z",
     "start_time": "2024-08-05T08:43:15.815994Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import datasets\n",
    "ds=datasets.load_dataset(\"nvidia/HelpSteer2\",split=\"train\")\n",
    "ds"
   ],
   "id": "449f1488ad0c5d48",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'response', 'helpfulness', 'correctness', 'coherence', 'complexity', 'verbosity'],\n",
       "    num_rows: 20324\n",
       "})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:47:26.254994Z",
     "start_time": "2024-08-05T08:47:26.011510Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import random\n",
    "idx=random.randint(0,len(ds)-1)\n",
    "print(f\"prompt:\\n\\n{ds['prompt'][idx]}\\n\\n\")\n",
    "print(f\"response:\\n\\n{ds['response'][idx]}\")"
   ],
   "id": "9bf6246dcaf0cbc2",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'ds' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mrandom\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m idx\u001B[38;5;241m=\u001B[39mrandom\u001B[38;5;241m.\u001B[39mrandint(\u001B[38;5;241m0\u001B[39m,\u001B[38;5;28mlen\u001B[39m(\u001B[43mds\u001B[49m)\u001B[38;5;241m-\u001B[39m\u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mprompt:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mprompt\u001B[39m\u001B[38;5;124m'\u001B[39m][idx]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mresponse:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mds[\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresponse\u001B[39m\u001B[38;5;124m'\u001B[39m][idx]\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "\u001B[1;31mNameError\u001B[0m: name 'ds' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:19.828076Z",
     "start_time": "2024-08-05T08:43:19.774528Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def format_prompt(example):\n",
    "    prompt_template=\"Below is a instruction that describes a task. Write a response that appropriately completes the request.\\n\\n### Instruction:\\n{instruction}\\n\\n### Response:\\n{response}\"   \n",
    "    return prompt_template.format(instruction=example[\"prompt\"],response=example[\"response\"])\n",
    "\n",
    "ds=ds.map(lambda x:{\"formated_prompt\":format_prompt(x)})"
   ],
   "id": "fc6169a567a54154",
   "outputs": [],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:25.947105Z",
     "start_time": "2024-08-05T08:43:19.829593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ds=ds.shuffle()\n",
    "ds=ds.map(lambda x:tokenizer(x[\"formated_prompt\"]),batched=True)"
   ],
   "id": "f18f779db1ea8d33",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Map:   0%|          | 0/20324 [00:00<?, ? examples/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "766dbe895f2c48029e4ad1fb856a8776"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:26.011874Z",
     "start_time": "2024-08-05T08:43:25.948617Z"
    }
   },
   "cell_type": "code",
   "source": "dataset=ds.train_test_split(test_size=.3,shuffle=True)",
   "id": "b4b184073b5dcf5b",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:26.015356Z",
     "start_time": "2024-08-05T08:43:26.012888Z"
    }
   },
   "cell_type": "code",
   "source": [
    "train_dataset=dataset[\"train\"]\n",
    "test_dataset=dataset[\"test\"]"
   ],
   "id": "9f6c85f60db1a218",
   "outputs": [],
   "execution_count": 26
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training ",
   "id": "794dd5a8c60ad275"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:43:26.024957Z",
     "start_time": "2024-08-05T08:43:26.017373Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model.print_trainable_parameters()",
   "id": "988c04445c70e213",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,766,720 || all params: 2,635,108,608 || trainable%: 0.7881\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:45:04.062418Z",
     "start_time": "2024-08-05T08:45:04.057788Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "import transformers \n",
    "\n",
    "def get_trainer(peft_model):\n",
    "    training_args=transformers.TrainingArguments(\n",
    "        per_device_train_batch_size=1,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_ratio=0.3,\n",
    "        max_steps=80,\n",
    "        learning_rate=2e-4,\n",
    "        num_train_epochs=1,\n",
    "        logging_steps=1,\n",
    "        logging_dir=\"./logs\",\n",
    "        output_dir=\"outputs\",\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        save_strategy=\"epoch\")\n",
    "\n",
    "    trainer=SFTTrainer(peft_model,\n",
    "                   train_dataset=train_dataset,\n",
    "                   eval_dataset=test_dataset,\n",
    "                   peft_config=lora_config,\n",
    "                   dataset_text_field=\"formated_prompt\",\n",
    "                   max_seq_length=1024,\n",
    "                   tokenizer=tokenizer,\n",
    "                   args=training_args,\n",
    "                   data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False))\n",
    "    return trainer"
   ],
   "id": "8ff0edef159ef7a8",
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:45:09.289407Z",
     "start_time": "2024-08-05T08:45:09.271676Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# for memory efficiency\n",
    "model=prepare_model_for_kbit_training(peft_model,use_gradient_checkpointing=True)\n",
    "\n",
    "# empty gpu cache\n",
    "torch.cuda.empty_cache()\n"
   ],
   "id": "fa26b8afddcaff77",
   "outputs": [],
   "execution_count": 31
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "trainer=get_trainer(peft_model)",
   "id": "1643c32db1bdddf0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-08-05T08:14:12.093020Z"
    }
   },
   "cell_type": "code",
   "source": "trainer.train()",
   "id": "1543b3ef49d55f02",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-04T18:14:02.866417Z",
     "start_time": "2024-08-04T18:14:02.866417Z"
    }
   },
   "cell_type": "code",
   "source": [
    "new_model_id=model_id+\"-instruct-HelpSteer2\"\n",
    "trainer.save_model(new_model_id)"
   ],
   "id": "4c41b60ab37a45a1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Second training ",
   "id": "246fb367dd9c6858"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:45:28.948616Z",
     "start_time": "2024-08-05T08:45:27.103754Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from peft import LoraConfig,get_peft_model,PeftConfig,PeftModel\n",
    "checkpoint=\"outputs/checkpoint-80\"\n",
    "from transformers import AutoModelForCausalLM\n",
    "peft_config=PeftConfig.from_pretrained(r\"outputs/checkpoint-80\")\n",
    "model = AutoModelForCausalLM.from_pretrained(peft_config.base_model_name_or_path)\n",
    "peft_model=PeftModel.from_pretrained(model,checkpoint,is_trainable=True)"
   ],
   "id": "1c4fa699e9b38bf1",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e8cfd3b6759f4a81a6ad22bdaa56d29b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 32
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:45:50.288215Z",
     "start_time": "2024-08-05T08:45:50.279994Z"
    }
   },
   "cell_type": "code",
   "source": "peft_model.print_trainable_parameters()",
   "id": "bde88b71c1380431",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 20,766,720 || all params: 2,635,108,608 || trainable%: 0.7881\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-08-05T08:46:26.722699Z",
     "start_time": "2024-08-05T08:45:55.660610Z"
    }
   },
   "cell_type": "code",
   "source": "second_trainer=get_trainer(peft_model)",
   "id": "56df2e5dc5562fe9",
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mRuntimeError\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[34], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m second_trainer\u001B[38;5;241m=\u001B[39m\u001B[43mget_trainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpeft_model\u001B[49m\u001B[43m)\u001B[49m\n",
      "Cell \u001B[1;32mIn[30], line 18\u001B[0m, in \u001B[0;36mget_trainer\u001B[1;34m(peft_model)\u001B[0m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_trainer\u001B[39m(peft_model):\n\u001B[0;32m      5\u001B[0m     training_args\u001B[38;5;241m=\u001B[39mtransformers\u001B[38;5;241m.\u001B[39mTrainingArguments(\n\u001B[0;32m      6\u001B[0m         per_device_train_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m1\u001B[39m,\n\u001B[0;32m      7\u001B[0m         gradient_accumulation_steps\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m4\u001B[39m,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m         optim\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mpaged_adamw_8bit\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     16\u001B[0m         save_strategy\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mepoch\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m---> 18\u001B[0m     trainer\u001B[38;5;241m=\u001B[39m\u001B[43mSFTTrainer\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpeft_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     19\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     20\u001B[0m \u001B[43m                   \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtest_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     21\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mpeft_config\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mlora_config\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     22\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mdataset_text_field\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mformated_prompt\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     23\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mmax_seq_length\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m1024\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m     24\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     25\u001B[0m \u001B[43m                   \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m     26\u001B[0m \u001B[43m                   \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtransformers\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mDataCollatorForLanguageModeling\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmlm\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mFalse\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     27\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m trainer\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001B[0m, in \u001B[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m     99\u001B[0m         message \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m custom_message\n\u001B[0;32m    100\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(message, \u001B[38;5;167;01mFutureWarning\u001B[39;00m)\n\u001B[1;32m--> 101\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\trl\\trainer\\sft_trainer.py:413\u001B[0m, in \u001B[0;36mSFTTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics, peft_config, dataset_text_field, packing, formatting_func, max_seq_length, infinite, num_of_sequences, chars_per_token, dataset_num_proc, dataset_batch_size, neftune_noise_alpha, model_init_kwargs, dataset_kwargs, eval_packing)\u001B[0m\n\u001B[0;32m    407\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mpadding_side \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m tokenizer\u001B[38;5;241m.\u001B[39mpadding_side \u001B[38;5;241m!=\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[0;32m    408\u001B[0m     warnings\u001B[38;5;241m.\u001B[39mwarn(\n\u001B[0;32m    409\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou passed a tokenizer with `padding_side` not equal to `right` to the SFTTrainer. This might lead to some unexpected behaviour due to \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    410\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124moverflow issues when training a model in half-precision. You might consider adding `tokenizer.padding_side = \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mright\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m` to your code.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    411\u001B[0m     )\n\u001B[1;32m--> 413\u001B[0m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[0;32m    414\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    415\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    416\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    417\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    418\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    419\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    420\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    421\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    422\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    423\u001B[0m \u001B[43m    \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    424\u001B[0m \u001B[43m    \u001B[49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    425\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    427\u001B[0m \u001B[38;5;66;03m# Add tags for models that have been loaded with the correct transformers version\u001B[39;00m\n\u001B[0;32m    428\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mmodel, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124madd_model_tags\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\transformers\\trainer.py:535\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[1;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[0;32m    530\u001B[0m \u001B[38;5;66;03m# Bnb Quantized models doesn't support `.to` operation.\u001B[39;00m\n\u001B[0;32m    531\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m (\n\u001B[0;32m    532\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mplace_model_on_device\n\u001B[0;32m    533\u001B[0m     \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mgetattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mquantization_method\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;28;01mNone\u001B[39;00m) \u001B[38;5;241m==\u001B[39m QuantizationMethod\u001B[38;5;241m.\u001B[39mBITS_AND_BYTES\n\u001B[0;32m    534\u001B[0m ):\n\u001B[1;32m--> 535\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_move_model_to_device\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    537\u001B[0m \u001B[38;5;66;03m# Force n_gpu to 1 to avoid DataParallel as MP will manage the GPUs\u001B[39;00m\n\u001B[0;32m    538\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mis_model_parallel:\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\transformers\\trainer.py:782\u001B[0m, in \u001B[0;36mTrainer._move_model_to_device\u001B[1;34m(self, model, device)\u001B[0m\n\u001B[0;32m    781\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_move_model_to_device\u001B[39m(\u001B[38;5;28mself\u001B[39m, model, device):\n\u001B[1;32m--> 782\u001B[0m     model \u001B[38;5;241m=\u001B[39m \u001B[43mmodel\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;66;03m# Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\u001B[39;00m\n\u001B[0;32m    784\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mparallel_mode \u001B[38;5;241m==\u001B[39m ParallelMode\u001B[38;5;241m.\u001B[39mTPU \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(model, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtie_weights\u001B[39m\u001B[38;5;124m\"\u001B[39m):\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1174\u001B[0m, in \u001B[0;36mModule.to\u001B[1;34m(self, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1171\u001B[0m         \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   1172\u001B[0m             \u001B[38;5;28;01mraise\u001B[39;00m\n\u001B[1;32m-> 1174\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mconvert\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "    \u001B[1;31m[... skipping similar frames: Module._apply at line 780 (5 times)]\u001B[0m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:780\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    778\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m recurse:\n\u001B[0;32m    779\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m module \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mchildren():\n\u001B[1;32m--> 780\u001B[0m         \u001B[43mmodule\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_apply\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfn\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    782\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mcompute_should_use_set_data\u001B[39m(tensor, tensor_applied):\n\u001B[0;32m    783\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m torch\u001B[38;5;241m.\u001B[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001B[0;32m    784\u001B[0m         \u001B[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001B[39;00m\n\u001B[0;32m    785\u001B[0m         \u001B[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    790\u001B[0m         \u001B[38;5;66;03m# global flag to let the user control whether they want the future\u001B[39;00m\n\u001B[0;32m    791\u001B[0m         \u001B[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:805\u001B[0m, in \u001B[0;36mModule._apply\u001B[1;34m(self, fn, recurse)\u001B[0m\n\u001B[0;32m    801\u001B[0m \u001B[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001B[39;00m\n\u001B[0;32m    802\u001B[0m \u001B[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001B[39;00m\n\u001B[0;32m    803\u001B[0m \u001B[38;5;66;03m# `with torch.no_grad():`\u001B[39;00m\n\u001B[0;32m    804\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m torch\u001B[38;5;241m.\u001B[39mno_grad():\n\u001B[1;32m--> 805\u001B[0m     param_applied \u001B[38;5;241m=\u001B[39m \u001B[43mfn\u001B[49m\u001B[43m(\u001B[49m\u001B[43mparam\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    806\u001B[0m p_should_use_set_data \u001B[38;5;241m=\u001B[39m compute_should_use_set_data(param, param_applied)\n\u001B[0;32m    808\u001B[0m \u001B[38;5;66;03m# subclasses may have multiple child tensors so we need to use swap_tensors\u001B[39;00m\n",
      "File \u001B[1;32m~\\Desktop\\projects\\finetune_gemma\\env\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1160\u001B[0m, in \u001B[0;36mModule.to.<locals>.convert\u001B[1;34m(t)\u001B[0m\n\u001B[0;32m   1153\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m convert_to_format \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mand\u001B[39;00m t\u001B[38;5;241m.\u001B[39mdim() \u001B[38;5;129;01min\u001B[39;00m (\u001B[38;5;241m4\u001B[39m, \u001B[38;5;241m5\u001B[39m):\n\u001B[0;32m   1154\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m t\u001B[38;5;241m.\u001B[39mto(\n\u001B[0;32m   1155\u001B[0m             device,\n\u001B[0;32m   1156\u001B[0m             dtype \u001B[38;5;28;01mif\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_floating_point() \u001B[38;5;129;01mor\u001B[39;00m t\u001B[38;5;241m.\u001B[39mis_complex() \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   1157\u001B[0m             non_blocking,\n\u001B[0;32m   1158\u001B[0m             memory_format\u001B[38;5;241m=\u001B[39mconvert_to_format,\n\u001B[0;32m   1159\u001B[0m         )\n\u001B[1;32m-> 1160\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mto\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m   1161\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdevice\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1162\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdtype\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_floating_point\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mis_complex\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[0;32m   1163\u001B[0m \u001B[43m        \u001B[49m\u001B[43mnon_blocking\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1164\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1165\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mNotImplementedError\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[0;32m   1166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mstr\u001B[39m(e) \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot copy out of meta tensor; no data!\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n",
      "\u001B[1;31mRuntimeError\u001B[0m: CUDA error: out of memory\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "second_trainer.train()",
   "id": "79d8e3d068ef987"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "new_model_id=model_id+\"-instruct-HelpSteer2\"\n",
    "trainer.save_model(new_model_id)"
   ],
   "id": "f922575895e16a28"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
